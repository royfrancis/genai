---
title: Models
description: Guide to various LLM models, features and benchmarks
execute:
  echo: false
  eval: true
---

```{r}
library(readxl)
library(gt)
library(highcharter)
library(lubridate)
library(htmltools)
library(dplyr)
```

## Overview

The table below lists the most common/popular LLMs.

```{r}
#| label: tbl-llms
#| tbl-cap: List of recent LLMs. Params in either known or estimated number of parameters in billions. Size is approximate model size in GB. Input is supported input modalities, Context is the maximum context window size in tokens.
#| output: asis

remove_css <- function(x) {
  x <- gsub("<style>.*</style>", "", x)
  htmltools::HTML(x)
}

readxl::read_xlsx("models.xlsx") %>%
  filter(!is.na(Release)) %>%
  dplyr::select(-Cost_Input, -Cost_Output) %>%
  mutate(Release = dmy(Release)) %>%
  arrange(desc(Release)) %>%
  select(-Comment) %>%
  gt(auto_align = FALSE) %>%
  cols_align(
    align = "left",
    columns = "Model"
  ) %>%
  opt_interactive(use_filters = TRUE)
# as_raw_html(inline_css = FALSE) %>%
# remove_css()
```

The graph below shows a timeline of popular LLMs and their intelligence over time. GPQA score is explained in the next section. This graph won't display well on narrow screens.

```{r}
#| eval: false
#| label: fig-scatter-llms
#| fig-cap: Timeline of recent LLMs showing number of hyperparameters on the Y axis. The size of the points denote the context window size in tokens.

dfr <- readxl::read_xlsx("models.xlsx") %>%
  mutate(Release = dmy(Release)) %>%
  filter(Release > "2022-01-01") %>%
  mutate(Company = factor(Company)) %>%
  mutate(Params = as.numeric(Params))

highchart(type = "chart") %>%
  hc_add_series(dfr, "scatter", hcaes(x = Release, y = Params, size = Params, group = Company),
    dataLabels = list(enabled = TRUE, format = "{point.Model}", style = list(textOutline = "none", color = "black")),
    minSize = 10, maxSize = 40
  ) %>%
  hc_xAxis(title = list(text = ""), type = "datetime") %>%
  hc_yAxis(title = list(text = "Number of hyperparameters (Billions)")) %>%
  hc_tooltip(pointFormat = "<b>{point.Model}</b>: {point.y}") %>%
  hc_chart(zoomType = "xy")
```

<div style="border: none; overflow: hidden; margin: 15px auto;">
<iframe scrolling="no" src="https://llm-stats.com/" style="border: 0px none; height: 1210px; margin-left: -80px; margin-top: -720px; width: 120%;">
</iframe>
</div>

The intelligence of a model is loosely based on the model architecture, number of hyperparameters, training data and more. The training methodology and the quality and quantity of training data may allow a smaller model to catch up to the capabilities of a larger model. 

Other factors that make a model desirable are speed and cost of running the model. Certain models may also be strong in certain focus areas such as coding, reasoning and knowledge.

## Benchmarking

[LLM benchmarks](https://www.confident-ai.com/blog/llm-benchmarks-mmlu-hellaswag-and-beyond) are a set of standardized tests designed to evaluate the performance of LLMs on various skills, such as reasoning and comprehension, and utilize specific scorers or metrics to quantitatively measure these abilities. 

These are some of the benchmark standards/datasets.

- [GPQA Diamond](https://github.com/idavidrein/gpqa) (Science)  
  Evaluates ability to answer challenging graduate-level questions in biology, chemistry, and physics. These questions are google-proof and requires deep spealized knowledge in the respective fields. Human experts achieve around 65% accuracy on this benchmark. Tests a model's ability to understand and apply complex domain-specific scientific concepts.
- [MMLU](https://zilliz.com/glossary/mmlu-benchmark) (Multi-task)  
  57 tasks designed to evaluate general knowledge and problem-solving capabilities of LLMs across diverse subjects spanning topics like humanities, STEM, social sciences, and more. Evaluates models in zero-shot and few-shot settings. Models are scored based on their accuracy in answering multiple-choice questions.
- [MMLU-Pro]() (Multi-task)  
  An enchanced version of MMLU with more challenging and complex questions. More reasoning intensive questions decreasing chance of gguessing. Less sensitive tto prompt variation.
- [MMMLU](https://huggingface.co/datasets/openai/MMMLU) (Multi-task)  
  Same as MMLU but multilingual
- [MMMU](https://mmmu-benchmark.github.io/) (Multimodal)  
  11500 questions covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering
- [SWE-bench](https://www.swebench.com/) (Agentic coding)  
  Evaluates if LLMs can evaluate real-world GitHub issues. Measures Agentic reasoning
- [Terminal-bench](https://www.tbench.ai/) (Agentic coding)  
  A collection of tasks to evaluate terminal mastery
- [TAU-bench](https://sierra.ai/blog/benchmarking-ai-agents) (Agentic tool usage)  
  Measures an agentâ€™s ability to interact with (simulated) human users and programmatic APIs while following domain-specific policies in a consistent manner. 
- [HellaSwag](https://rowanzellers.com/hellaswag/) (Reasoning)  
  Language completion task that evaluates commonsense reasoning
- [Humanity's Last Exam](https://agi.safe.ai/) (Reasoning)  
  A multi-modal benchmark with 2500 challenging questions across over a hundred subjects to evaluate reasoning and knowledge
- [AIME](https://www.vals.ai/benchmarks/aime-2025-03-11) (Mathematics)  
  Competitive high school math benchmark
- Math 500 (Mathematics)  
  Evaluates mathematical problem solving from high school to competition-level problems. Includes algebra, geometry, probability, and calculus.
- [BFCL](https://gorilla.cs.berkeley.edu/leaderboard.html)  
  Measures how LLMs use tools
- [Aider polyglot](https://aider.chat/docs/leaderboards/) (Coding)  
  Measures capabilities for writing and editing code
- LiveCodeBench (Coding)  
  Measures capabilities for code editing

Here are some leaderboards that rank LLMs.

- [LLM Stats](https://llm-stats.com/)
- [Vellum](https://www.vellum.ai/llm-leaderboard)
- [ArtificialAnalysis](https://artificialanalysis.ai/)
- [WebDev Arena](https://web.lmarena.ai/leaderboard)
- [LM Arena](https://lmarena.ai/?leaderboard)
- [EQ Bench](https://eqbench.com/)
- [HF LymSys](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)
- [TrustBit](https://www.trustbit.tech/en/llm-leaderboard-juli-2024)
- [BFCL](https://gorilla.cs.berkeley.edu/leaderboard.html)
- [DocsBot](https://docsbot.ai/models) Model specs

## Image models

Here is a list of the top image generation models

- [GPT Image 1](https://platform.openai.com/docs/models/gpt-image-1) | OpenAI, Closed
- [Flux](https://blackforestlabs.ai/) | Blackforst labs, Open source
- [Imagen 3](https://deepmind.google/technologies/imagen-3/) | Deepmind, Closed
- [Grok 2](https://x.ai/blog/grok-2) | xAI, Closed
- [GPT-4](https://openai.com/index/gpt-4/) | OpenAI, Closed
- [Dall-E 3](https://openai.com/index/dall-e-3/) | OpenAI, Closed
- [Stable diffusion](https://stability.ai/stable-image) | Stability AI, Open source
- [Firefly](https://firefly.adobe.com/) | Adobe, Closed
- [Midjourney](https://www.midjourney.com/showcase) | Midjourney, Closed
- [Amazon Nova Canvas](https://www.aboutamazon.com/news/aws/amazon-nova-artificial-intelligence-bedrock-aws) | Amazon, Closed

## Video models

- [Sora](https://openai.com/sora/) | OpenAI, Closed
- [Veo 2](https://deepmind.google/technologies/veo/veo-2/) | Google Deepmind, Closed
- [Amazon Nova Reel](https://aws.amazon.com/ai/generative-ai/nova/creative/) | Amazon, Closed

## Music models

- Lyria | Google Deepmind, Closed
